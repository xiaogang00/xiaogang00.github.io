---
layout: post
title: PRML Chapter1 Introduction
category: PRML
tags: machine_learning
---

* content
{:toc}




### 1. 多项式曲线拟合的例子

对$sin(2 \cdot \pi \cdot x)$曲线进行多项式拟合，根据sina函数均匀生成带高斯noise的点，作为训练集合以及测试集合。多项式函数是关于w的线性函数，是一种重要的模型，称为线性模型，在第三、四章会详细介绍，与kernal方法结合，是模式识别和机器学习最重要的方法之一。 

定义了错误函数：预测值和实际值之间的差的平方和，这个也是在高斯noise下后验概率的估计结果，作为最小化优化的目标函数，求出w。错误函数是关于w的二次函数，所以有唯一的全局最优解，求导数即可以得到w。

书中以多项式的阶次为例说明了模型选择和模型对比的概念。指出模式识别和机器学习的最重要的问题是模型的泛化能力。

在测试集上定义了错误函数EMS(root-mean-square error): 



$$
EMS = sqrt( 2 \cdot E(w) / N )
$$


书中给出了M（阶数）的不同值下，可以发现当M很大的时候，高阶多项式函数可以精确地与数据匹配，为了拟合训练集合的点，系数会出现很大的正数和负数，导致曲线有比较大的波动，从而导致数据点之间的点拟合比较差（$sin (2\pi x)$ 的幂级数展开包含所有阶数的项，所以我们可能会以为结果会随着 M 的增⼤⽽单调地变好）：有着更⼤的 M 值的更灵活的多项式被过分地调参，使得多项式被调节成了与⽬标值的随机噪声相符；也就是当M越大越目标值就越容易产生随机的噪音。

**解决方法**：

1. 当训练集的规模扩大时，复杂的模型的过度拟合问题就会缓解，并达到更好的效果。一般来说训练集元素的数量要是参数个数的5到10倍以上。当然模型的复杂性不仅仅反映在参数的个数上。同样我们不应该根据训练集的大小来选择模型，而应该根据问题的复杂性来选择合适的模型。 
2. 最小平方方法是最大似然估计的一个特例，过度拟合也是最大似然估计的一个固有特点。采用贝叶斯方法，过度拟合的问题就会被克服，因为贝叶斯模型的有效参数是根据 训练集合的大小自动调整的。 
3. 采用正则化/规范化（regularization的）方法来解决过度拟合的问题。错误函数中添加了 $\frac{1}{2}  \cdot \lambda \cdot  sqrt( \parallel W \parallel )$来避免W中出现过大或过小的正负数值系数。通过对比 选择合适的$\lambda$值来解决过度拟合的问题。 



### 2. 概率论

广泛使用的基于频率型估计的是最大似然估计。w被估计为使似然函数$P(D\mid w)$取得最大值的w。 

贝叶斯观点的一个重要的优势是很自然的集成了先验知识：比如一玫均匀的硬币抛掷了三次，都是正面朝上，使用最大似然估计方法，正面朝上的概率为1，但是贝叶斯集成合理的先验概率能够产生不那么极端的结论。 

贝叶斯方法通常被批评先验分布通常是选择基于数学上便利的而不是反应事实的先验信念。基于一个不好的先验可能会给出更差的结果。 通过交叉验证的技术可以评测模型之间的好坏。 

高斯分布： 高斯分布又成为正态分布，是一种重要的连续型变量的分布。分布函数、期望和方差。 



### 3. 模型选择

从曲线拟合的例子中，我们可以看到有一个最佳的M似的模型具有最好的泛化能力。多项式的阶控制着多项式自由参数的个数，因此控制着模型的复杂性。我们经常会考虑一系列的不同的类型的模型，然后根据特定的应用选择一个最好的模型，这就是模型选择问题。 

书中提出了交叉验证的方法（leave-one-out）来选择模型的参数或者模型



### 4. 维度灾难

在多项式拟合的例子只有一个输入参数x，而在实际中我们可能处理高维的多个输入参数，这将会带来巨大的挑战。 书中举了一个类似k临近的分类的例子，意思是把输入的空间划分成一个个的格子空间，然后统计要药分类数据所在的格子的点数量最多的类别为该点的类别。这个随着空间的增加，格子的数量程指数级别增加。 关于多变量的例子：曲线拟合的例子如果输入变量为D个，那么多项式的系数呈$D^M$增长，其中M为多项式的阶。 



### 5. 决策论

结合我们前面说的概率论，决策理论能够然我们在不确定的情况下做出最优的决策。 介绍了最小化分类错误率、最小化期望损失。

拒绝区间：如果在一个区间内很难做出决策，那么最好别拒绝做出决策，而是让人去做。可以对$p(C_k \mid x)$设置一个threshold。 
Inference and decision：给出了三种方式： 

1. 生成模型方式：首先建模分布$p(x,C_k)$，然后得到条件概率，然后做出决策；
2. 判别模型：直接对后验概率$p(C_k \mid x)$进行建模，然后做出决策；
3. 直接给出判别函数，然后将输入直接映射为分类的label。 

如果直接对于分类决策问题，使用1方法比较浪费计算资源和需要过多的训练数据，因为联合概率分布可能有很多和后验概率不相干的结构。使用2方法是一个比较直接的好方法。组合1和2方法是机器学习方法现在研究 比较多的。 方法3无法得到后验概率，这将会有很多问题。



### 6. 信息论 

我们给一个离散的随机变量x,当我们观测到这个变量的一些值之后，我们想问我们得到了多少信息。信息的多少可以按照“惊奇度”来度量，越是不可能的事件发生，越能给出更多的信息，一定要发生的事件给的信息量为0. 所以度量信息两药依赖于概率分布p(x)，所以我们希望找到一种度量，他是p(x)的单调的函数。我们观察两个独立的变量x，y，那么他们的信息量应该是单独观察这两个变量信息量的和，即：



$$
h(x,y) = h(x) + h(y)
$$


而$$p(x,y) = p(x) \cdot p(y)$$满足这种关系的函数h必须是对数形式： 



$$
h(x) = -log(p(x))
$$
在通信模型中，增益代表数据的2进制编码长度，所以取2为底的对数。在其他则可以去其他的对数形式，比如自然对数。

相对增益 : 对于一个不知道的分布p(x),我们使用一个近似的分布q(x)来建模，那么如果我们使用q(x)来编码数据，那么对于指定的x，平均需要多传输的信息。 书中使用了凸函数的性质，证明$$KL(p \parallel q) > 0$$

所以我们可以使用相对增益来描述分布p(x)和q(x)的不相似性。 
互信息： 描述两个随机变量接近相互独立的程度： 



$$
I[x, y] = KL(p(x, y) \parallel p(x)p(y))
$$

