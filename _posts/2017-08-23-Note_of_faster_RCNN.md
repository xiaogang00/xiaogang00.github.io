---
layout: post
title: Learning note for faster RCNN
category: Computer Vision
tags: computer_vision
---

* content
{:toc}





在RCNN和fast RCNN之后，目标检测界的领军人物Ross Girshick团队在2015年又提出了faster RCNN。在这个框架下，简单网络目标检测速度达到17fps，在PASCAL VOC上准确率为59.9%；复杂网络达到5fps，准确率78.8%。







### 主要的思想

从RCNN到fast RCNN，再到本文的faster RCNN，目标检测的四个基本步骤（候选区域生成，特征提取，分类，位置精修）终于被**统一到一个深度网络框架之内**。所有计算没有重复，完全在GPU中完成，大大提高了运行速度。这三个框架可以由下图进行比较说明：

![faster_RCNN]({{ site.url }}/images/faster_RCNN.png) 

faster RCNN可以简单地看做“区域生成网络+fast RCNN“的系统，用区域生成网络代替fast RCNN中的Selective Search方法。论文着重解决了这个系统中的三个问题： 

1. 如何设计区域生成网络 
2. 如何训练区域生成网络 
3. 如何让区域生成网络和fast RCNN网络**共享特征提取网络**



### 算法细节

#### 区域生成网络：结构

基本设想是：在提取好的特征图上，对所有可能的候选框进行判别。由于后续还有位置精修步骤，所以候选框实际比较稀疏。 



#### 特征提取

原始特征提取（上图灰色方框）包含若干层conv+relu，直接套用ImageNet上常见的分类网络即可。本文试验了两种网络：5层的ZF，16层的VGG-16，具体结构不再赘述。 额外添加一个conv+relu层，输出51*39*256维特征（feature）。



#### 候选区域(anchor)

特征可以看做一个尺度$51\times 39$的256通道图像，对于该图像的每一个位置，考虑9个可能的候选窗口：三种面积${1282,2562,5122} \times $三种比例${1:1,1:2,2:1}$。这些候选窗口称为anchors。下图示出$51\times 39$个anchor中心，以及9种anchor示例。 

![anchor]({{ site.url }}/images/anchor.png)

在整个faster RCNN算法中，有三种尺度:

1. 原图尺度：原始输入的大小。不受任何限制，不影响性能。 
2. 归一化尺度：输入特征提取网络的大小。anchor在这个尺度上设定。这个参数和anchor的相对大小决定了想要检测的目标范围。 
3. 网络输入尺度：输入特征检测网络的大小，在训练时设置，源码中$224\times 224$。



#### 窗口分类和位置精修

分类层(cls_score)输出每一个位置上，9个anchor属于前景和背景的概率；窗口回归层(bbox_pred)输出每一个位置上，9个anchor对应窗口应该平移缩放的参数。 对于每一个位置来说，分类层从256维特征中输出属于前景和背景的概率；窗口回归层从256维特征中输出4个平移缩放参数。



### 区域生成网络：训练

#### 样本设置

1. 对每个标定的真值候选区域，与其重叠比例最大的anchor记为前景样本 ；

2. 对(1)之后剩余的anchor，如果其与某个标定重叠比例大于0.7，记为前景样本；如果其与任意一个标定的重叠比例都小于0.3，记为背景样本 ；

3. 对(2)，(3)剩余的anchor，弃去不用；

4. 跨越图像边界的anchor弃去不用。

   ​

#### 代价函数

同时最小化两种代价： 分类误差，前景样本的窗口位置偏差。



#### 共享特征

区域生成网络（RPN）和fast RCNN都需要一个原始特征提取网络。这个网络使用ImageNet的分类库得到初始参数$W_0$，本文讲解了三种方法来精调参数。

1. 从$W_0$开始，训练RPN。用RPN提取训练集上的候选区域 ；
2. 从$W_0$开始，用候选区域训练Fast RCNN，参数记为$W_1$ ；
3. 从$W_1$开始，训练RPN。

具体操作时，仅执行两次迭代，并在训练时冻结了部分层。论文中的实验使用此方法。
