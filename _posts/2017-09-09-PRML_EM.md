---
layout: post
title: PRML Chapter9 Mixture Models and EM
category: PRML
tags: machine_learning
---

* content
{:toc}






### EM算法

Probabilistic model 中有 observed variables $X$ 和 latent variables $Z$。他们的联合概率为：$p(X,Z\mid \theta)$，由参数$\theta$控制。所有我们的问题目标就是寻找参数$\theta$，是的对observed data 的似然最大化：$p(X\mid \theta) = \sum\limits_Z p(X,Z\mid \theta)$

我们看到直接对于$p(X \mid \theta)$求最值是困难的，但是对于 complete-data likelihood $p(X,Z\mid \theta)$相对比较容易，但是在这里还有问题就是Z是观察不到的。

**EM算法**

我们引入一个分部$q(Z)$，从而把对数似然$ln [p(X\mid \theta)]$进行分解：


$$
ln [p(X\mid \theta)] = L(q, \theta) + KL(q \parallel p)
$$
其中的$L(q,\theta) = \sum\limits_Z q(Z) ln\{ \frac{p(X,Z\mid \theta)}{q(Z)} \}$

EM算法分两步进行迭代（我们假设$\theta$当前的值是$\theta^{old}$）

1. 在固定$\theta^{old}$的情况下，maximize $L (q,\theta)$ with respect to $q(Z)$ 。这需要使得$q(Z) = p(Z\mid X, \theta^{old})$，这也就是E step；

2. 在固定$q(Z) = p(Z\mid X, \theta^{old})$的情况下，maximize  $L (q,\theta)$ with respect to $\theta$ 。随之我们可以得到一个新的参数$\theta^{new}$，这也就是M step。

   ​

在上述的迭代中，在E step下，$ln[p(X\mid \theta)]$没有改变，因为这一步主要是针对$q(Z)$进行优化；在M step中，$ln[p(X \mid \theta)]$由于新的参数而增大，因为$L(q, \theta^{new}) \geq L(q, \theta^{old})$，因为在新的参数$\theta^{new}$下，$q(Z)$不等于$p(Z\mid X, \theta^{new})$，所以$KL(q \parallel p)$大于0。这样 EM 算法每迭代一次，参数都在向着目标前进。

进一步解释 EM 算法：

1. $ln[p(X\mid \theta)]$与$q(Z)$无关，因为任意改变$q(Z)$不影响$ln[p(X\mid \theta)]$的值。所以关于$q(Z)$对$L(q, \theta)$的最大化，我们只需要使得$q(Z) = p(Z\mid X ,\theta)$，也就是$KL(q\parallel p) =0$。

2. 在M step中，我们固定$q(Z)$为（ 观察到 X 后） Z 的后验概率后:
   $$
   L(q,\ theta) = \sum\limits_Z p(Z \mid X, \theta^{old}) ln[p(X, Z\mid \theta)] + const
   $$
   所以 maximize $L(q,\theta)$with respect to $\theta$ 等价于最大化：
   $$
   Q(\theta , \theta^{old}) = \sum\limits_Z p(Z \mid X, \theta^{old}) ln[p(X, Z\mid \theta)]
   $$
   respect to $\theta$ 。而$Q(\theta, \theta^{old})$正是 complete-data likelihood $ln[p(X,Z\mid \theta)]$（ 观察到X后的）Z 后验概率 $p(Z\mid X,\theta^{old}）$下的期望。可见， M step 的 Maximize 的正就是 complete-data likelihood 的期望

以上 EM 求解的是（使 observed variables 的）似然最大化的参数，这属于 Frequentist 的框架；这暗示着还有 Bayesian 的 EM。



### Gaussian Mixture Model（ GMM）

首先， Gaussian mixture distribution 是指分布：$p(x) = \sum\limits_{k=1}^K \pi_k N(x\mid \mu_k, \Sigma_k)$。可以验证这的确是一个分布，但是不属于Exponential Family。

我们假设random vector z是一个1-of-K的编码方式，其分布是$p(z) = \prod\limits_{k=1}^K \pi_k ^{z_k}$。其中$0\leq \pi_k \leq 1，\sum\limits_{k=1}^K \pi_k= 1$。

假设有分布$p(x \mid z_k = 1) = N(x\mid \mu_k,\Sigma_k)$，那么我们可以得到x的边缘分布:


$$
p(x) = \sum\limits_zp(z)p(x\mid z) = \sum\limits_{k=1}^K \pi_k N(x\mid \mu_k, \Sigma_k)
$$


GMM 的问题是：当得 到 N 个 x 的观察后，确定的 Gaussian mixture distribution 中的参数。以上问题对应到 EM 的思路中： z 是 latent variable， x 是 observed variable。假设得到 N 个观察，记为 X，其中每行是一个观察值 x；每个观察值都会有一个相应的 latent variable 值，记为 Z，同样也是每行一个 z。

根据这些 observation，需要对 Gaussian mixture distribution 进行 inference，找到其中的三大组参数：$\pi = \{ \pi_k:k = 1,\dots,K \}, \mu = \{ \mu_k:k=1,\dots,K \},\Sigma = \{ \Sigma_k:k=1,\dots,K \}$

一般的GMM计算过程:

假设我们已经有参数$\pi^{old}, \mu^{old}, \Sigma^{old}$，那么算法就是两步的循环迭代：第一步是计算这组参数下面，在z的后验概率下$z_{nk} $的期望$\gamma(z_{nk})$，第二部就是计算$E_Z[p(X,Z\mid \pi, \mu, \Sigma)]$关于三个参数$\pi, \mu, \Sigma$的最大值，从而可以得到新的参数$\pi^{new}, \mu^{new}, \Sigma^{new}$。

其中$\gamma(z_{nk}) = E[z_{nk}] = \frac{\pi_k N(x_n \mid \mu_k, \Sigma_k)}{\sum\limits_{j=1}^K \pi_j N(x_n\mid \mu_j,\Sigma_j)}$



而且有:


$$
E_Z[p(X,Z\mid \pi,\mu, \Sigma)] = \sum\limits_{n=1}^K \sum\limits_{k=1}^K \gamma(z_{nk})\{ ln\pi_k + lnN(x_n\mid \mu_k, \Sigma_k) \}
$$


### GMM观点的k-means

这个观点简单概括就是： k-means 是 GMM 的一个特例。
首先应该认识到，对于 clustering，k-means 是一个 hard assignment of data points to clusters；但是 GMM 是一个 soft 的 assignment，因为它计算的是一个后验概率$\gamma(z_{nk})$。

假设有一个 GMM，每个分支的均值向量是$\mu_k$ ，协方差矩阵是$\epsilon I$。也就是，每个分支有各自的均值，但是它们的协方差相同。 代入上面 GMM 的公式，可以得到：


$$
\gamma(z_{nk}) = \frac{\pi_k exp\{ -\parallel x_n-\mu_k \parallel^2 / 2\epsilon \}}{\sum\limits_{j=1}^K \pi_j exp\{ -\parallel x_n - \mu_j \parallel ^2/ 2\epsilon \}}
$$


假设在所有的K个$\parallel x_n - \mu_j \parallel$，最小的一个是$\parallel x_n - \mu_{j^*} \parallel$。上式的分子分母同时除以$exp\{ -\parallel x_n - \mu_{j^*} \parallel^2 / 2\epsilon \}$，我们可以发现如果$k \neq j^*$ ，分母为1，分子为0；如果$k = j^*$，分子和分母都为1。



这表明，$(\lim\limits_{\epsilon \to 0}\gamma(z_{n1}),\dots,\lim\limits_{\epsilon \to 0}\gamma(z_{nK}))​$成为了一个了一个 1-of-K 编码方式。 这时候 GMM 也成为了一个 hard assignment 的聚类了。



